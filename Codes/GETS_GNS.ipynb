{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GETS Specification Search with GNS DGP\n",
    "\n",
    "\n",
    "The purpose of this template is to implement and analyze the Elhorst-Vega GETS strategy, starting with the estimation of the GNS specification. Varying values for $\\gamma$, $\\rho$ and $\\lambda$ are considered. This includes spatial Durbin models ($\\lambda = 0$), SLX error models ($\\rho = 0$), standard SLX regression ($\\rho = \\lambda = 0$), and for $\\gamma = 0$, the standard spatial error ($\\rho = 0$), spatial lag ($\\lambda = 0$ and standard regression model ($\\rho = \\lambda = 0$).\n",
    "\n",
    "The true DGP is GNS, i.e., using dgp_gns. Model estimation depends on the branch in the decision tree. All decisions are based on p-values for the individual t-tests.\n",
    "\n",
    "This design is any spatial layout.\n",
    "\n",
    "Note: frequencies are adjusted for actual succesful runs, i.e., out of bounds parameters are removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2\n",
    "import time\n",
    "import spreg\n",
    "import libpysal\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font\n",
    "from openpyxl.formatting.rule import CellIsRule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pd \",pd.__version__)\n",
    "print(\"gpd \",gpd.__version__)\n",
    "print(\"np \",np.__version__)\n",
    "print(\"spreg \",spreg.__version__)\n",
    "print(\"libpysal \",libpysal.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Common Factor Hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comfac_test(betas,vm):\n",
    "    \"\"\"\n",
    "    Computes the Spatial Common Factor Hypothesis test as shown in Anselin (1988, p. 226-229)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    betas       : array\n",
    "                  coefficient estimates in Spatial Durbin model constant, beta, gamma, lambda\n",
    "    vm          : array\n",
    "                  Variance-covariance matrix of the coefficients in Spatial Durbin model\n",
    "                  Order matches theta' = [constant, beta', gamma', lambda]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Wld     : float\n",
    "              Wald statistic\n",
    "    pvalue  : float\n",
    "              P value for Wald statistic calculated as a Chi sq. distribution\n",
    "              with degrees of freedom equal to k-1 (with k as the number of betas)\n",
    "\n",
    "    \"\"\"\n",
    "    kall = betas.shape[0]\n",
    "    k = int((kall - 2)/2)\n",
    "    k1 = int(1 + k)\n",
    "    b = betas[1:k1]\n",
    "    gam = betas[k1:k1+k]\n",
    "    lam = betas[-1]\n",
    "    vv = vm[1:,1:]\n",
    "    g = lam*b + gam\n",
    "    G = np.vstack((lam*np.eye(b.shape[0]),np.eye(b.shape[0]),b.T))\n",
    "\n",
    "    GVGi = np.linalg.inv(np.dot(G.T, np.dot(vv, G)))\n",
    "    Wld = np.dot(g.T, np.dot(GVGi, g))[0][0]\n",
    "    df = k\n",
    "    pvalue = 1-chi2.cdf(Wld, df)\n",
    "    return Wld, pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GETS Specification Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_spec(y,x, w, wlags= 2, p_value=0.05):\n",
    "    \"\"\"\n",
    "    Backward specification: Starting from the estimation of General Nested Model, \n",
    "                            it imposes constraints and test significance of\n",
    "                            coefficients to suggest the most appropriate model\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    x: matrix of independent variables\n",
    "    y: vector of dependent variable\n",
    "    w: spatial weights matrix \n",
    "    wlags: number of spatial lags to use in S2SLS\n",
    "    p_value= significance threshold\n",
    "        \n",
    "    Returns:\n",
    "    ----------\n",
    "    result: the suggested DGP according to the backward (general to specific) specification search\n",
    "    steps: the number of steps required to reach the final suggest\n",
    "    \"\"\"\n",
    "\n",
    "    # methods = ['OLS','SEM','SAR','SDM','SLX','SAREr','SLXEr', 'GNS']\n",
    "\n",
    "    p=p_value\n",
    "    \n",
    "    k = x.shape[1]\n",
    "    model_gns=spreg.GMM_Error(y,x,w=w,slx_lags=1,add_wy=True,w_lags=wlags)\n",
    "    pstats = np.array(model_gns.z_stat)[1+k:,1]\n",
    "    pk = len(pstats)   # number of p-values, last one is p_lam, next to last p_rho, before that p_gam\n",
    "    \n",
    "    if pstats.max() < p:   # least significant of three is still significant\n",
    "        result='GNS'\n",
    "        paths=1\n",
    "    elif pstats.min() >= p:  # all non-significant\n",
    "            result='OLS'\n",
    "            paths=1\n",
    "    else:       # at least one non-significant and one sig spatial parameter\n",
    "                # since max is not sig, but (at least) min is\n",
    "        cand = pstats.argmax()   # least significant is not sig since max > p\n",
    "        if cand == (pk-1):    # lambda not significant, but at least one of rho/gamma is\n",
    "        # go to spatial Durbin - only rho and gam\n",
    "            model_spd = spreg.GM_Lag(y,x,w=w,slx_lags=1,w_lags=wlags)\n",
    "            pstats = np.array(model_spd.z_stat)[1+k:,1]\n",
    "            pk = len(pstats)\n",
    "            if pstats.max() < p:  # least significant of two is still significant\n",
    "                # check on spatial common factor\n",
    "                Wld,pwld = comfac_test(model_spd.betas,model_spd.vm)\n",
    "                if pwld < p:  # reject common factor hypothesis\n",
    "                    result='SDM'\n",
    "                    paths=2\n",
    "                else:\n",
    "                    result='SEM'\n",
    "                    paths=2\n",
    "            elif pstats.min() >= p:  # none significant, even bother?\n",
    "                result='OLS'\n",
    "                paths=2\n",
    "            else:  # one significant and one non-sign spatial parameter\n",
    "                cand = pstats.argmax()  # non-significant one\n",
    "                if cand == (pk - 1):   # rho not sig\n",
    "                    result='SLX'\n",
    "                    paths=3\n",
    "                else: # gamma not sig\n",
    "                    result='SAR'\n",
    "                    paths=3\n",
    "        elif cand == (pk-2):   # rho not significant, but at least one of lambda/gamma is\n",
    "            # go to SLX-Error\n",
    "            model_slxerr = spreg.GMM_Error(y,x,w=w,slx_lags=1)\n",
    "            pstats == np.array(model_slxerr.z_stat)[1+k:,1]\n",
    "            pk = len(pstats)\n",
    "            if pstats.max() < p:  # least significant of two is still significant\n",
    "                result='SLXEr'\n",
    "                paths=4\n",
    "            elif pstats.min() >= p:  # none significant, even bother?\n",
    "                result='OLS'\n",
    "                paths=4\n",
    "            else:  # one significant and one non-sign spatial parameter\n",
    "                cand = pstats.argmax()  # non-significant one\n",
    "                if cand == (pk - 1):   # lambda not sig\n",
    "                    result='SLX'\n",
    "                    paths=5\n",
    "                else: # gamma not sig\n",
    "                    result='SEM'\n",
    "                    paths=5\n",
    "        else:   # gamma not sig, but at least one of rho/lambda is\n",
    "            # go to SARSAR\n",
    "            model_sarsar = spreg.GMM_Error(y,x,w=w,add_wy=True,w_lags=wlags)\n",
    "            pstats = np.array(model_sarsar.z_stat)[1+k:,1]\n",
    "            pk = len(pstats)\n",
    "            if pstats.max() < p:  # least significant of two is still significant\n",
    "                result='SAREr'\n",
    "                paths=6\n",
    "            elif pstats.min() >= p:  # none significant, even bother?\n",
    "                result='OLS'\n",
    "                paths=6\n",
    "            else:  # one significant and one non-sign spatial parameter\n",
    "                cand = pstats.argmax()  # non-significant one\n",
    "                if cand == (pk - 1):   # lambda not sig\n",
    "                    result='SAR'\n",
    "                    paths=7\n",
    "                else: # rho not sig\n",
    "                    result='SEM'\n",
    "                    paths=7\n",
    "    return(result,paths)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Data and Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20x20 square grid - queen contiguity - n=400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#infileshp = \"./data_master/twentwengrid.shp\"\n",
    "#infilew = \"./data_master/grid400_q.gal\"\n",
    "#layout = \"20x20\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 40x40 square grid - queen contiguity - n=1600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#infileshp = \"./data_master/fourty40grid.shp\"\n",
    "#infilew = \"./data_master/fourty40_q.gal\"\n",
    "#layout = \"40x40\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US Counties - queen contiguity - n=3085"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infileshp = \"./data_master/uscounty_nodata.shp\"\n",
    "infilew = \"./data_master/uscounty_q.gal\"\n",
    "layout = \"US_counties\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brazilian Municipios - queen contiguity - n=5568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#infileshp = \"./data_master/Brazil_nodata.shp\"\n",
    "#infilew = \"./data_master/Braz_muni_q.gal\"\n",
    "#layout = \"BRA_muni\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = gpd.read_file(infileshp)\n",
    "\n",
    "print(dfs.shape)\n",
    "print(list(dfs))\n",
    "\n",
    "w = libpysal.io.open(infilew).read()\n",
    "w.transform = 'r'\n",
    "n = w.n\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters set here:\n",
    "\n",
    "- sample size (n)\n",
    "- number of replications\n",
    "- types of weights\n",
    "- type of error process (SAR or MA)\n",
    "- beta and gamma parameters\n",
    "- rho and lambda ranges\n",
    "\n",
    "Note: the DGP and estimator are hard coded for efficiency (otherwise the if condition would have to be evaluated for each replication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall random seed\n",
    "rndseed = 123456789\n",
    "# number of replications\n",
    "reps=1000\n",
    "# error process\n",
    "errp = 'sar'\n",
    "#errp = 'ma'\n",
    "# beta and gamma\n",
    "b1 = [1,1]\n",
    "#b1 = [1, 1, 1, 1]\n",
    "# rho range and lambda range\n",
    "rho_values = [0, 0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "lam_values = [0, 0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "# gamma range\n",
    "gam_values = [0, -0.5, 0.5]\n",
    "# result parameter labels\n",
    "models = ['OLS','SEM','SAR','SDM','SLX','SAREr','SLXEr','GNS']\n",
    "# Nested Dictionary to store results with the structure Modselect [gam][rho][lam] with model acronym\n",
    "Modselect = {gam: {rho: {lam: {model: np.zeros(reps) for model in models} for lam in lam_values} \n",
    "                   for rho in rho_values} for gam in gam_values}\n",
    "# Nested dictionary with path selection for Modpaths[gam][rho][lam]\n",
    "Modpaths = {gam: {rho: {lam: np.zeros(reps,dtype=int) for lam in lam_values} \n",
    "                   for rho in rho_values} for gam in gam_values}# \n",
    "# Nested dictionary with number of model exceptions for Modexcept[gam][rho][lam]\n",
    "Modexcept = {gam: {rho: {lam: 0 for lam in lam_values} \n",
    "                   for rho in rho_values} for gam in gam_values}\n",
    "# error-vals not needed, inferred from paths = 0\n",
    "#error_vals = []\n",
    "\n",
    "# inverse method - alternative is 'true_inv'\n",
    "invmethod = 'power_exp'\n",
    "# p-value\n",
    "pvalue = 0.01\n",
    "#pvalue = 0.05\n",
    "# error distribution\n",
    "errdist = 'normal'\n",
    "#errdist = 'lognormal'\n",
    "\n",
    "# number of explanatory variables\n",
    "kx = len(b1) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RHS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nk = n*kx\n",
    "var1 = 12.0/kx\n",
    "rng=np.random.default_rng(seed=rndseed) # set for X\n",
    "xx = spreg.dgp.make_x(rng,nk,mu=[0],varu=[var1],method=\"uniform\")\n",
    "if kx > 1:\n",
    "    x1 = np.reshape(xx,(n,kx))\n",
    "else:\n",
    "    x1 = xx\n",
    "xb1 = spreg.dgp.make_xb(x1,b1)\n",
    "wx1 = spreg.dgp.make_wx(x1,w) # default first order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SETTINGS - GETS Search with GNS DGP - Elhorst/Vega approach\")\n",
    "print(\"Layout: \",infileshp)\n",
    "print(\"Weights: \",infilew)\n",
    "print(\"n: \",n)\n",
    "print(\"k: \",kx)\n",
    "print(\"Error Process: \",errp)\n",
    "print(\"Error Distribution: \",errdist)\n",
    "print(\"Replications: \",reps)\n",
    "print(\"p-value: \",pvalue)\n",
    "print(\"Inverse Method: \",invmethod)\n",
    "print(\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "if errdist == 'normal':\n",
    "    vv = 6.0     # var 6 for target R2 of 0.66\n",
    "elif errdist == 'lognormal':\n",
    "    vv = 1.1     # var 1.1 for target R2 of 0.66\n",
    "else:\n",
    "    print(\"Error distribution not recognized\")     # not used\n",
    "\n",
    "\n",
    "for gam in gam_values:\n",
    "    gg=gam\n",
    "    # create a list with multiple gamma values (all same) when more than one x\n",
    "    if kx > 1:\n",
    "        g1 = np.ones(kx)*gg\n",
    "        g1 = g1.tolist()\n",
    "    else:\n",
    "        g1 = gg\n",
    "    #print(\"gam \",g1)\n",
    "    wxg1 = spreg.dgp.make_wxg(wx1,g1) \n",
    "    for rho in rho_values:\n",
    "        rho1=rho\n",
    "        #print(\"rho \",rho1)\n",
    "        for lam in lam_values:\n",
    "            lam1=lam\n",
    "            #print(\"lam \",lam1)\n",
    "            if not(rho1 + lam1 < 1):\n",
    "                break\n",
    "            else:\n",
    "                print(g1,rho1,lam1)\n",
    "                rng=np.random.default_rng(seed=rndseed) # reset for simulations\n",
    "                for i in range (reps):  # no resampling for out of range parameters\n",
    "                    try:                        \n",
    "                        u= spreg.dgp.make_error(rng,n,mu=0,varu=vv,method=errdist)  # error distribution as an option\n",
    "\n",
    "                        # DGP is GNS\n",
    "                        y1 = spreg.dgp_gns(u,xb1,wxg1,w,rho1,lam1, model= errp)\n",
    "                        # Run backward specification\n",
    "                        model_suggested,paths = back_spec(y1, xb1, w)\n",
    "                        #print(\"back spec \",model_suggested,\" paths \",paths)\n",
    "                        # Append result\n",
    "                        Modselect[gam][rho][lam][model_suggested][i] = 1\n",
    "                        Modpaths[gam][rho][lam][i]=paths\n",
    "                    except:  \n",
    "                        #error_vals.append([gam,rho,lam])\n",
    "                        #print(\"except\")\n",
    "                        Modpaths[gam][rho][lam][i]=0\n",
    "                        \n",
    "\n",
    "t1 = time.time()\n",
    "print(\"time in minutes: \",(t1-t0)/60.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gam in gam_values:\n",
    "    print(\"Gam: \",gam)\n",
    "    for rho in rho_values:\n",
    "        for lam in lam_values:\n",
    "            if (rho + lam < 1):\n",
    "                pfreq = np.zeros(8,dtype=int)  # holder for counts by path\n",
    "                vp = Modpaths[gam][rho][lam]\n",
    "                vals,counts = np.unique(vp,return_counts=True)\n",
    "                pfreq[vals]=counts\n",
    "                Modexcept[gam][rho][lam]=pfreq[0]\n",
    "                print(\" Rho: \",rho,\" Lam: \",lam,\" Path Counts: \",pfreq)\n",
    "                #print(\"Modexcept \",Modexcept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph with Selection Frequencies by Model Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenr = len(rho_values)\n",
    "lenl = len(lam_values)\n",
    "for gam in gam_values:\n",
    "    print(\"GAMMA: \",gam)\n",
    "    print(\"------------\")\n",
    "    for pt in range(len(models)):\n",
    "        mod = models[pt]\n",
    "        modsel = np.zeros((lenr,lenl))\n",
    "        for r in range(lenr):\n",
    "            rr = lenr - 1 -r\n",
    "            for c in range(lenl):\n",
    "                rho = rho_values[r]\n",
    "                lam = lam_values[c]\n",
    "                if not(rho+lam < 1):\n",
    "                    modsel[rr,c]= np.nan\n",
    "                else:    # divide model selection count by valid models not reps\n",
    "                    #modsel[rr,c] = np.mean(Modselect[gam][rho][lam][mod])\n",
    "                    modpicks = Modselect[gam][rho][lam][mod]\n",
    "                    #print(\" modpicks \",modpicks)\n",
    "                    realreps = reps - Modexcept[gam][rho][lam]\n",
    "                    #print(\"realreps \",realreps)\n",
    "                    if realreps > 0: # check for zero reps\n",
    "                        modsel[rr,c]= modpicks.sum() / realreps\n",
    "                    else:\n",
    "                        modsel[rr,c]= np.nan\n",
    "        print(\"Selection Frequency for\",mod)\n",
    "        print(modsel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_models={}\n",
    "# Save dictionary values in lists\n",
    "\n",
    "for mod in models:\n",
    "    data = []\n",
    "    for gam in gam_values:\n",
    "        for rho in rho_values:\n",
    "            for lam in lam_values:\n",
    "                if rho + lam < 1:  \n",
    "                    modpicks = Modselect[gam][rho][lam][mod]\n",
    "                    realreps = reps - Modexcept[gam][rho][lam]\n",
    "                    result= modpicks.sum() / realreps\n",
    "                    data.append({'gamma': gam, 'rho': rho, 'lambda': lam, 'value': result})\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    # Pivot the DataFrame to get the desired shape\n",
    "    pivot_df = df.pivot_table(index=['lambda', 'rho'], columns='gamma', values='value', aggfunc='first')\n",
    "    pivot_df = pivot_df.reset_index()[['rho', 'lambda', 0, -0.5, 0.5]]\n",
    "    # Save the DataFrame in the dictionary, following a specific order for the excel file\n",
    "    results_models[mod] = pivot_df.iloc[[0,1,2,3,4,5,6,11,15,18,20,7,8,9,10,12,13,14,16,17,19]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save in Excel format\n",
    "\n",
    "with pd.ExcelWriter(f'005_GETS_GNS_{layout}_{errp}_{errdist}_p_{pvalue}.xlsx', engine='openpyxl') as writer:\n",
    "\n",
    "    ws = writer.book.create_sheet(title='Sheet1')\n",
    "\n",
    "    startrow = 0  # Initial row to start writing the dataframe\n",
    "    for name, df in results_models.items():\n",
    "        \n",
    "        # Write the dataframe name\n",
    "        ws.cell(row=startrow + 1, column=1).value = f\"{name}\"\n",
    "        \n",
    "        # Write the dataframe content\n",
    "        df.round(3).to_excel(writer, sheet_name='Sheet1', startrow=startrow + 1, index=False)       \n",
    "        # Update the startrow for the next dataframe\n",
    "        endrow = startrow + 2 + len(df)\n",
    "        startrow += len(df) + 3  \n",
    "    \n",
    "    # Apply conditional formatting to columns 3, 4, and 5 for specific value ranges\n",
    "    # Define fonts for each color\n",
    "    red_font = Font(color='FF0000')  \n",
    "    wine_font = Font(color='722F37')   \n",
    "    blue_font = Font(color='0000FF')  \n",
    "    green_font = Font(color='00FF00') \n",
    "    columns = ['C', 'D', 'E']  # Corresponding to Excel columns 3, 4, and 5\n",
    "    for col in columns:\n",
    "        \n",
    "        # Rule for Red: value > 0.95\n",
    "        ws.conditional_formatting.add(f'{col}3:{col}{endrow}',\n",
    "                                      CellIsRule(operator='greaterThan', formula=['0.95'], stopIfTrue=True, font=red_font))\n",
    "        # Rule for Wine: 0.9 < value <= 0.95\n",
    "        ws.conditional_formatting.add(f'{col}3:{col}{endrow}',\n",
    "                                      CellIsRule(operator='between', formula=['0.9', '0.95'], stopIfTrue=True, font=wine_font))\n",
    "        # Rule for Blue: 0.75 < value <= 0.9\n",
    "        ws.conditional_formatting.add(f'{col}3:{col}{endrow}',\n",
    "                                      CellIsRule(operator='between', formula=['0.75', '0.9'], stopIfTrue=True, font=blue_font))\n",
    "        # Rule for Green: 0.5 < value <= 0.75\n",
    "        ws.conditional_formatting.add(f'{col}3:{col}{endrow}',\n",
    "                                      CellIsRule(operator='between', formula=['0.5001', '0.75'], stopIfTrue=True, font=green_font))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
